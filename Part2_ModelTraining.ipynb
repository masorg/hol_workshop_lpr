{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7a315f-0c97-40b9-85be-dd3dc7f230d0",
   "metadata": {},
   "source": [
    "# Part 2: License Plate Bounding Box Detection.\n",
    "\n",
    "**Goal**: Build a model that finds the exact location coordinates of license plates in images\n",
    "\n",
    "\n",
    "## **What We'll Build (Actually Interesting!):**\n",
    "1. ** Bounding Box Detector**: Predicts (x, y, width, height) coordinates\n",
    "2. ** Real Object Detection**: Not just \"is there a plate?\" but \"WHERE is the plate?\"\n",
    "3. ** Regression Model**: Outputs actual coordinate values\n",
    "4. ** Visual Results**: See detected boxes overlaid on images\n",
    "5. ** Production-Ready**: Downloadable detection model\n",
    "\n",
    "\n",
    "## **Technical Challenge:**\n",
    "- **Input**: 400x400 RGB image\n",
    "- **Output**: [x_center, y_center, width, height] normalized coordinates\n",
    "- **Loss**: Mean Squared Error for coordinate regression\n",
    "- **Evaluation**: IoU (Intersection over Union) with ground truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc5764-7ad0-4d95-81f1-73d0a53e95fa",
   "metadata": {},
   "source": [
    "## **Setup and Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c69c292-1290-4273-b804-40aa79ffd647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:09:00.657157: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-18 16:09:01.072353: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-18 16:09:01.176880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752854941.425817      90 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752854941.457904      90 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752854941.767435      90 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752854941.767461      90 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752854941.767463      90 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752854941.767465      90 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 16:09:01.772409: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PART 2: LICENSE PLATE BOUNDING BOX DETECTION\n",
      "=======================================================\n",
      " TensorFlow version: 2.19.0\n",
      "  GPU available: False\n",
      " Building REAL object detection (not boring classification)!\n",
      " Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:09:30.959194: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for bounding box detection\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import random\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Deep learning for regression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\" PART 2: LICENSE PLATE BOUNDING BOX DETECTION\")\n",
    "print(\"=\" * 55)\n",
    "print(f\" TensorFlow version: {tf.__version__}\")\n",
    "print(f\"  GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\" Building REAL object detection (not boring classification)!\")\n",
    "print(\" Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7637c-0877-483c-b724-4809d1f0e824",
   "metadata": {},
   "source": [
    "## **Load Dataset and Extract Bounding Boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74cda1ce-8667-4704-8a70-a895d51ff0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LOADING BOUNDING BOX DATASET\n",
      "===================================\n",
      " Dataset found!\n",
      "     Images: 433\n",
      "    Annotations: 433\n",
      "\n",
      " Sample bounding box extraction:\n",
      "    Cars112.png (240x400): 1 license plates\n",
      "       Plate 1: center=(0.444, 0.407), size=(0.479, 0.140)\n",
      "    Cars310.png (600x531): 1 license plates\n",
      "       Plate 1: center=(0.491, 0.740), size=(0.178, 0.090)\n",
      "    Cars31.png (400x245): 1 license plates\n",
      "       Plate 1: center=(0.858, 0.784), size=(0.285, 0.155)\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = \"lpr_data\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"annotations\")\n",
    "MODEL_SAVE_DIR = \"saved_models\"\n",
    "IMG_SIZE = 400  # Input size for model\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def parse_bounding_box(xml_path):\n",
    "    \"\"\"Extract bounding box coordinates from XML annotation\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get image dimensions\n",
    "        size = root.find('size')\n",
    "        img_width = int(size.find('width').text)\n",
    "        img_height = int(size.find('height').text)\n",
    "        \n",
    "        # Find license plate bounding boxes\n",
    "        bboxes = []\n",
    "        for obj in root.findall('object'):\n",
    "            name_elem = obj.find('name')\n",
    "            n_elem = obj.find('n')\n",
    "            \n",
    "            obj_name = \"\"\n",
    "            if name_elem is not None and name_elem.text:\n",
    "                obj_name = name_elem.text.lower()\n",
    "            elif n_elem is not None and n_elem.text:\n",
    "                obj_name = n_elem.text.lower()\n",
    "            \n",
    "            # Check if it's a license plate\n",
    "            if 'licen' in obj_name or 'plate' in obj_name:\n",
    "                bbox = obj.find('bndbox')\n",
    "                if bbox is not None:\n",
    "                    xmin = int(bbox.find('xmin').text)\n",
    "                    ymin = int(bbox.find('ymin').text)\n",
    "                    xmax = int(bbox.find('xmax').text)\n",
    "                    ymax = int(bbox.find('ymax').text)\n",
    "                    \n",
    "                    # Convert to center coordinates and normalize\n",
    "                    x_center = (xmin + xmax) / 2.0 / img_width\n",
    "                    y_center = (ymin + ymax) / 2.0 / img_height\n",
    "                    width = (xmax - xmin) / img_width\n",
    "                    height = (ymax - ymin) / img_height\n",
    "                    \n",
    "                    bboxes.append([x_center, y_center, width, height])\n",
    "        \n",
    "        return bboxes, img_width, img_height\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error parsing {xml_path}: {e}\")\n",
    "        return [], 0, 0\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=IMG_SIZE):\n",
    "    \"\"\"Load and resize image while maintaining aspect ratio\"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return None\n",
    "            \n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (target_size, target_size))\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\" LOADING BOUNDING BOX DATASET\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check dataset\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\" Dataset not found: {DATA_DIR}\")\n",
    "    print(\"📥 Please ensure you have the lpr_data directory\")\n",
    "else:\n",
    "    image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.png')] if os.path.exists(IMAGES_DIR) else []\n",
    "    annotation_files = [f for f in os.listdir(ANNOTATIONS_DIR) if f.endswith('.xml')] if os.path.exists(ANNOTATIONS_DIR) else []\n",
    "    \n",
    "    print(f\" Dataset found!\")\n",
    "    print(f\"     Images: {len(image_files)}\")\n",
    "    print(f\"    Annotations: {len(annotation_files)}\")\n",
    "    \n",
    "    # Process first few files to show what we're doing\n",
    "    if len(image_files) > 0:\n",
    "        print(f\"\\n Sample bounding box extraction:\")\n",
    "        for i in range(min(3, len(image_files))):\n",
    "            xml_file = image_files[i].replace('.png', '.xml')\n",
    "            xml_path = os.path.join(ANNOTATIONS_DIR, xml_file)\n",
    "            \n",
    "            if os.path.exists(xml_path):\n",
    "                bboxes, w, h = parse_bounding_box(xml_path)\n",
    "                print(f\"    {image_files[i]} ({w}x{h}): {len(bboxes)} license plates\")\n",
    "                for j, bbox in enumerate(bboxes):\n",
    "                    print(f\"       Plate {j+1}: center=({bbox[0]:.3f}, {bbox[1]:.3f}), size=({bbox[2]:.3f}, {bbox[3]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84347aa2-b408-41c4-b2e9-15b2248ed88c",
   "metadata": {},
   "source": [
    "##  **Prepare Training Data for Bounding Box Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d487aa1-d49b-4434-8635-12f7ee2597e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PREPARING BOUNDING BOX TRAINING DATA\n",
      "========================================\n",
      " Processing 433 images for bounding box training...\n",
      "    Processed 0/433 images...\n",
      "    Processed 100/433 images...\n",
      "    Processed 200/433 images...\n",
      "    Processed 300/433 images...\n",
      "    Processed 400/433 images...\n",
      "\n",
      " Training data prepared!\n",
      "    Total valid samples: 433\n",
      "    Image shape: (400, 400, 3)\n",
      "    Bbox shape: (4,)\n",
      "    Images with multiple plates: 24\n",
      "    Sample bounding boxes:\n",
      "       Sample 1: center=(0.444, 0.407), size=(0.479, 0.140)\n",
      "       Sample 2: center=(0.491, 0.740), size=(0.178, 0.090)\n",
      "       Sample 3: center=(0.858, 0.784), size=(0.285, 0.155)\n",
      "       Sample 4: center=(0.249, 0.750), size=(0.242, 0.144)\n",
      "       Sample 5: center=(0.321, 0.468), size=(0.188, 0.112)\n",
      "\n",
      "🔍 Data Quality Check:\n",
      "    X coordinate range: 0.034 - 0.949\n",
      "    Y coordinate range: 0.202 - 0.979\n",
      "    Width range: 0.022 - 0.972\n",
      "    Height range: 0.020 - 0.800\n"
     ]
    }
   ],
   "source": [
    "print(\" PREPARING BOUNDING BOX TRAINING DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare training data\n",
    "X_data = []  # Images\n",
    "y_data = []  # Bounding box coordinates [x_center, y_center, width, height]\n",
    "\n",
    "if len(image_files) > 0:\n",
    "    print(f\" Processing {len(image_files)} images for bounding box training...\")\n",
    "    \n",
    "    valid_samples = 0\n",
    "    multiple_plates = 0\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"    Processed {i}/{len(image_files)} images...\")\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "        image = load_and_preprocess_image(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        # Load bounding box\n",
    "        xml_file = img_file.replace('.png', '.xml')\n",
    "        xml_path = os.path.join(ANNOTATIONS_DIR, xml_file)\n",
    "        \n",
    "        if not os.path.exists(xml_path):\n",
    "            continue\n",
    "            \n",
    "        bboxes, orig_w, orig_h = parse_bounding_box(xml_path)\n",
    "        \n",
    "        if len(bboxes) == 0:\n",
    "            continue\n",
    "        elif len(bboxes) > 1:\n",
    "            multiple_plates += 1\n",
    "            # For simplicity, use the first bounding box (largest usually)\n",
    "            # In production, you'd use multi-object detection\n",
    "            bbox = bboxes[0]\n",
    "        else:\n",
    "            bbox = bboxes[0]\n",
    "        \n",
    "        # Add to training data\n",
    "        X_data.append(image)\n",
    "        y_data.append(bbox)  # [x_center, y_center, width, height] normalized\n",
    "        valid_samples += 1\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    print(f\"\\n Training data prepared!\")\n",
    "    print(f\"    Total valid samples: {len(X_data)}\")\n",
    "    print(f\"    Image shape: {X_data[0].shape}\")\n",
    "    print(f\"    Bbox shape: {y_data[0].shape}\")\n",
    "    print(f\"    Images with multiple plates: {multiple_plates}\")\n",
    "    print(f\"    Sample bounding boxes:\")\n",
    "    for i in range(min(5, len(y_data))):\n",
    "        x, y, w, h = y_data[i]\n",
    "        print(f\"       Sample {i+1}: center=({x:.3f}, {y:.3f}), size=({w:.3f}, {h:.3f})\")\n",
    "    \n",
    "    # Verify data quality\n",
    "    print(f\"\\n🔍 Data Quality Check:\")\n",
    "    print(f\"    X coordinate range: {y_data[:, 0].min():.3f} - {y_data[:, 0].max():.3f}\")\n",
    "    print(f\"    Y coordinate range: {y_data[:, 1].min():.3f} - {y_data[:, 1].max():.3f}\")\n",
    "    print(f\"    Width range: {y_data[:, 2].min():.3f} - {y_data[:, 2].max():.3f}\")\n",
    "    print(f\"    Height range: {y_data[:, 3].min():.3f} - {y_data[:, 3].max():.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\" No images found!\")\n",
    "    print(\" Creating dummy data for demonstration...\")\n",
    "    # Create synthetic data for demo\n",
    "    X_data = np.random.random((50, IMG_SIZE, IMG_SIZE, 3)).astype(np.float32)\n",
    "    y_data = np.random.random((50, 4)).astype(np.float32)  # Random normalized coordinates\n",
    "    print(f\" Dummy data created: {len(X_data)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c3479-e88a-41f4-8467-ebe629f3b9ae",
   "metadata": {},
   "source": [
    "##  **Build Bounding Box Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd7b7f9-92d1-4a4f-8f73-cf747ababb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BUILDING BOUNDING BOX REGRESSION MODEL\n",
      "=============================================\n",
      " Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m2,052\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,737,540</span> (21.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,737,540\u001b[0m (21.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,737,540</span> (21.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,737,540\u001b[0m (21.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Statistics:\n",
      "    Total parameters: 5,737,540\n",
      "    Model type: Bounding Box Regression CNN\n",
      "    Task: Predict [x_center, y_center, width, height]\n",
      "    Input: 400x400x3 RGB images\n",
      "    Output: 4 normalized coordinates [0,1]\n",
      "    Loss: MSE (Mean Squared Error)\n",
      "    Metrics: IoU (Intersection over Union) + MAE\n"
     ]
    }
   ],
   "source": [
    "def create_bbox_detector():\n",
    "    \"\"\"Create a CNN model for bounding box regression\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        \n",
    "        # Feature extraction backbone (similar to VGG-style)\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Regression head\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4, activation='sigmoid')  # Output: [x_center, y_center, width, height]\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def iou_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate IoU (Intersection over Union) metric for bounding boxes\"\"\"\n",
    "    # Convert from center coordinates to corner coordinates\n",
    "    def center_to_corner(boxes):\n",
    "        x_center, y_center, width, height = tf.split(boxes, 4, axis=-1)\n",
    "        x1 = x_center - width / 2\n",
    "        y1 = y_center - height / 2\n",
    "        x2 = x_center + width / 2\n",
    "        y2 = y_center + height / 2\n",
    "        return tf.concat([x1, y1, x2, y2], axis=-1)\n",
    "    \n",
    "    true_corners = center_to_corner(y_true)\n",
    "    pred_corners = center_to_corner(y_pred)\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x1 = tf.maximum(true_corners[..., 0], pred_corners[..., 0])\n",
    "    y1 = tf.maximum(true_corners[..., 1], pred_corners[..., 1])\n",
    "    x2 = tf.minimum(true_corners[..., 2], pred_corners[..., 2])\n",
    "    y2 = tf.minimum(true_corners[..., 3], pred_corners[..., 3])\n",
    "    \n",
    "    intersection = tf.maximum(0.0, x2 - x1) * tf.maximum(0.0, y2 - y1)\n",
    "    \n",
    "    # Calculate union\n",
    "    true_area = (true_corners[..., 2] - true_corners[..., 0]) * (true_corners[..., 3] - true_corners[..., 1])\n",
    "    pred_area = (pred_corners[..., 2] - pred_corners[..., 0]) * (pred_corners[..., 3] - pred_corners[..., 1])\n",
    "    union = true_area + pred_area - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / (union + 1e-7)\n",
    "    return tf.reduce_mean(iou)\n",
    "\n",
    "print(\" BUILDING BOUNDING BOX REGRESSION MODEL\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create the model\n",
    "model = create_bbox_detector()\n",
    "\n",
    "# Compile with appropriate loss and metrics for regression\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',  # Mean Squared Error for coordinate regression\n",
    "    metrics=[iou_metric, 'mae']  # IoU and Mean Absolute Error\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "print(\" Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"    Total parameters: {model.count_params():,}\")\n",
    "print(f\"    Model type: Bounding Box Regression CNN\")\n",
    "print(f\"    Task: Predict [x_center, y_center, width, height]\")\n",
    "print(f\"    Input: {IMG_SIZE}x{IMG_SIZE}x3 RGB images\")\n",
    "print(f\"    Output: 4 normalized coordinates [0,1]\")\n",
    "print(f\"    Loss: MSE (Mean Squared Error)\")\n",
    "print(f\"    Metrics: IoU (Intersection over Union) + MAE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522b623-df77-4908-8c26-520be333a82f",
   "metadata": {},
   "source": [
    "##  **Train the Bounding Box Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5e3e4-8635-42f8-9e10-94c9e701ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TRAINING BOUNDING BOX DETECTION MODEL\n",
      "=============================================\n",
      " Data Split:\n",
      "    Training samples: 346\n",
      "    Validation samples: 87\n",
      "\n",
      " Training coordinate ranges:\n",
      "    X: 0.060 - 0.949\n",
      "    Y: 0.202 - 0.979\n",
      "    W: 0.022 - 0.972\n",
      "    H: 0.020 - 0.800\n",
      "\n",
      " Starting bounding box regression training...\n",
      "Epoch 1/2\n",
      "\u001b[1m 1/22\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:32\u001b[0m 10s/step - iou_metric: 0.0896 - loss: 0.0771 - mae: 0.2381"
     ]
    }
   ],
   "source": [
    "print(\" TRAINING BOUNDING BOX DETECTION MODEL\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if len(X_data) > 10:  # Need sufficient data for training\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_data, y_data, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\" Data Split:\")\n",
    "    print(f\"    Training samples: {len(X_train)}\")\n",
    "    print(f\"    Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Verify coordinate ranges\n",
    "    print(f\"\\n Training coordinate ranges:\")\n",
    "    print(f\"    X: {y_train[:, 0].min():.3f} - {y_train[:, 0].max():.3f}\")\n",
    "    print(f\"    Y: {y_train[:, 1].min():.3f} - {y_train[:, 1].max():.3f}\")\n",
    "    print(f\"    W: {y_train[:, 2].min():.3f} - {y_train[:, 2].max():.3f}\")\n",
    "    print(f\"    H: {y_train[:, 3].min():.3f} - {y_train[:, 3].max():.3f}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\n Starting bounding box regression training...\")\n",
    "    \n",
    "    # Create callbacks for better training\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=2,  # More epochs for regression\n",
    "        batch_size=16,  # Smaller batch size for better gradients\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Get final metrics\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_iou = history.history['iou_metric'][-1]\n",
    "    final_val_iou = history.history['val_iou_metric'][-1]\n",
    "    final_train_mae = history.history['mae'][-1]\n",
    "    final_val_mae = history.history['val_mae'][-1]\n",
    "    \n",
    "    print(f\"\\n TRAINING COMPLETE!\")\n",
    "    print(f\"    Final Training Loss (MSE): {final_train_loss:.4f}\")\n",
    "    print(f\"    Final Validation Loss (MSE): {final_val_loss:.4f}\")\n",
    "    print(f\"    Final Training IoU: {final_train_iou:.4f}\")\n",
    "    print(f\"    Final Validation IoU: {final_val_iou:.4f}\")\n",
    "    print(f\"    Final Training MAE: {final_train_mae:.4f}\")\n",
    "    print(f\"    Final Validation MAE: {final_val_mae:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title(' Model Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # IoU plot\n",
    "    ax2.plot(history.history['iou_metric'], 'b-', label='Training IoU', linewidth=2)\n",
    "    ax2.plot(history.history['val_iou_metric'], 'r-', label='Validation IoU', linewidth=2)\n",
    "    ax2.set_title(' IoU Score', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Intersection over Union')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE plot\n",
    "    ax3.plot(history.history['mae'], 'b-', label='Training MAE', linewidth=2)\n",
    "    ax3.plot(history.history['val_mae'], 'r-', label='Validation MAE', linewidth=2)\n",
    "    ax3.set_title(' Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('MAE')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot (if available)\n",
    "    if 'lr' in history.history:\n",
    "        ax4.plot(history.history['lr'], 'g-', linewidth=2)\n",
    "        ax4.set_title(' Learning Rate', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Learning Rate')\n",
    "        ax4.set_yscale('log')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Learning Rate\\\\nNot Tracked', ha='center', va='center', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.set_title(' Learning Rate', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Store training results\n",
    "    training_results = {\n",
    "        'final_train_loss': float(final_train_loss),\n",
    "        'final_val_loss': float(final_val_loss),\n",
    "        'final_train_iou': float(final_train_iou),\n",
    "        'final_val_iou': float(final_val_iou),\n",
    "        'final_train_mae': float(final_train_mae),\n",
    "        'final_val_mae': float(final_val_mae),\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val)\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(\" Insufficient data for training!\")\n",
    "    print(f\"    Current samples: {len(X_data)}\")\n",
    "    print(\"    Need at least 10 samples for meaningful training\")\n",
    "    training_results = {'status': 'insufficient_data'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cd7f8-63de-49a3-894a-9d129398800d",
   "metadata": {},
   "source": [
    "##  **Visualize Predictions vs Ground Truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb59d5-dfde-4798-b4c4-e1b2aa09d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bbox_predictions(images, true_bboxes, pred_bboxes, num_samples=6):\n",
    "    \"\"\"Visualize predicted vs ground truth bounding boxes\"\"\"\n",
    "    \n",
    "    num_samples = min(num_samples, len(images))\n",
    "    cols = 3\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(images[i])\n",
    "        \n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        img_h, img_w = images[i].shape[:2]\n",
    "        \n",
    "        # Ground truth box (green)\n",
    "        true_bbox = true_bboxes[i]\n",
    "        true_x_center, true_y_center, true_width, true_height = true_bbox\n",
    "        true_x1 = (true_x_center - true_width / 2) * img_w\n",
    "        true_y1 = (true_y_center - true_height / 2) * img_h\n",
    "        true_w = true_width * img_w\n",
    "        true_h = true_height * img_h\n",
    "        \n",
    "        true_rect = patches.Rectangle(\n",
    "            (true_x1, true_y1), true_w, true_h,\n",
    "            linewidth=3, edgecolor='green', facecolor='none',\n",
    "            label='Ground Truth'\n",
    "        )\n",
    "        ax.add_patch(true_rect)\n",
    "        \n",
    "        # Predicted box (red)\n",
    "        pred_bbox = pred_bboxes[i]\n",
    "        pred_x_center, pred_y_center, pred_width, pred_height = pred_bbox\n",
    "        pred_x1 = (pred_x_center - pred_width / 2) * img_w\n",
    "        pred_y1 = (pred_y_center - pred_height / 2) * img_h\n",
    "        pred_w = pred_width * img_w\n",
    "        pred_h = pred_height * img_h\n",
    "        \n",
    "        pred_rect = patches.Rectangle(\n",
    "            (pred_x1, pred_y1), pred_w, pred_h,\n",
    "            linewidth=3, edgecolor='red', facecolor='none',\n",
    "            label='Prediction'\n",
    "        )\n",
    "        ax.add_patch(pred_rect)\n",
    "        \n",
    "        # Calculate IoU for this sample\n",
    "        def calculate_iou_single(box1, box2):\n",
    "            # Convert to corner coordinates\n",
    "            x1_1 = box1[0] - box1[2] / 2\n",
    "            y1_1 = box1[1] - box1[3] / 2\n",
    "            x2_1 = box1[0] + box1[2] / 2\n",
    "            y2_1 = box1[1] + box1[3] / 2\n",
    "            \n",
    "            x1_2 = box2[0] - box2[2] / 2\n",
    "            y1_2 = box2[1] - box2[3] / 2\n",
    "            x2_2 = box2[0] + box2[2] / 2\n",
    "            y2_2 = box2[1] + box2[3] / 2\n",
    "            \n",
    "            # Calculate intersection\n",
    "            x1_i = max(x1_1, x1_2)\n",
    "            y1_i = max(y1_1, y1_2)\n",
    "            x2_i = min(x2_1, x2_2)\n",
    "            y2_i = min(y2_1, y2_2)\n",
    "            \n",
    "            if x2_i <= x1_i or y2_i <= y1_i:\n",
    "                return 0.0\n",
    "            \n",
    "            intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "            area1 = box1[2] * box1[3]\n",
    "            area2 = box2[2] * box2[3]\n",
    "            union = area1 + area2 - intersection\n",
    "            \n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        iou = calculate_iou_single(true_bbox, pred_bbox)\n",
    "        \n",
    "        ax.set_title(f'Sample {i+1}\\\\nIoU: {iou:.3f}', fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add legend to first subplot\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(num_samples, rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\" VISUALIZING BOUNDING BOX PREDICTIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if len(X_data) > 10 and 'X_val' in locals():\n",
    "    # Get predictions on validation set\n",
    "    print(\" Getting predictions on validation set...\")\n",
    "    \n",
    "    val_predictions = model.predict(X_val, verbose=0)\n",
    "    \n",
    "    # Calculate IoU for all validation samples\n",
    "    ious = []\n",
    "    for i in range(len(X_val)):\n",
    "        true_bbox = y_val[i]\n",
    "        pred_bbox = val_predictions[i]\n",
    "        \n",
    "        # Calculate IoU\n",
    "        def calculate_iou_single(box1, box2):\n",
    "            x1_1 = box1[0] - box1[2] / 2\n",
    "            y1_1 = box1[1] - box1[3] / 2\n",
    "            x2_1 = box1[0] + box1[2] / 2\n",
    "            y2_1 = box1[1] + box1[3] / 2\n",
    "            \n",
    "            x1_2 = box2[0] - box2[2] / 2\n",
    "            y1_2 = box2[1] - box2[3] / 2\n",
    "            x2_2 = box2[0] + box2[2] / 2\n",
    "            y2_2 = box2[1] + box2[3] / 2\n",
    "            \n",
    "            x1_i = max(x1_1, x1_2)\n",
    "            y1_i = max(y1_1, y1_2)\n",
    "            x2_i = min(x2_1, x2_2)\n",
    "            y2_i = min(y2_1, y2_2)\n",
    "            \n",
    "            if x2_i <= x1_i or y2_i <= y1_i:\n",
    "                return 0.0\n",
    "            \n",
    "            intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "            area1 = box1[2] * box1[3]\n",
    "            area2 = box2[2] * box2[3]\n",
    "            union = area1 + area2 - intersection\n",
    "            \n",
    "            return intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        iou = calculate_iou_single(true_bbox, pred_bbox)\n",
    "        ious.append(iou)\n",
    "    \n",
    "    mean_iou = np.mean(ious)\n",
    "    print(f\" Validation Set Performance:\")\n",
    "    print(f\"    Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"    Best IoU: {max(ious):.4f}\")\n",
    "    print(f\"    Worst IoU: {min(ious):.4f}\")\n",
    "    print(f\"    IoU > 0.5: {sum(1 for iou in ious if iou > 0.5)}/{len(ious)} ({100*sum(1 for iou in ious if iou > 0.5)/len(ious):.1f}%)\")\n",
    "    \n",
    "    # Visualize best and worst predictions\n",
    "    print(f\"\\n Showing validation predictions:\")\n",
    "    \n",
    "    # Sort by IoU to show best and worst\n",
    "    sorted_indices = np.argsort(ious)\n",
    "    \n",
    "    # Show a mix of best and worst predictions\n",
    "    display_indices = []\n",
    "    display_indices.extend(sorted_indices[-3:])  # Best 3\n",
    "    display_indices.extend(sorted_indices[:3])   # Worst 3\n",
    "    \n",
    "    display_images = X_val[display_indices]\n",
    "    display_true = y_val[display_indices]\n",
    "    display_pred = val_predictions[display_indices]\n",
    "    \n",
    "    visualize_bbox_predictions(display_images, display_true, display_pred, 6)\n",
    "    \n",
    "    # Update training results with IoU\n",
    "    training_results['mean_validation_iou'] = float(mean_iou)\n",
    "    training_results['best_validation_iou'] = float(max(ious))\n",
    "    training_results['worst_validation_iou'] = float(min(ious))\n",
    "    \n",
    "else:\n",
    "    print(\" No validation data available for visualization\")\n",
    "    print(\"   Creating demo visualization...\")\n",
    "    \n",
    "    # Create dummy visualization\n",
    "    demo_images = np.random.random((3, 400, 400, 3))\n",
    "    demo_true = np.array([[0.5, 0.5, 0.3, 0.2], [0.6, 0.4, 0.25, 0.15], [0.4, 0.6, 0.35, 0.25]])\n",
    "    demo_pred = demo_true + np.random.normal(0, 0.05, demo_true.shape)\n",
    "    demo_pred = np.clip(demo_pred, 0, 1)  # Keep in valid range\n",
    "    \n",
    "    visualize_bbox_predictions(demo_images, demo_true, demo_pred, 3)\n",
    "    print(\" Demo visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9af51-83b9-4f25-a20d-5bbca2c3e90e",
   "metadata": {},
   "source": [
    "## **Save and Download Bounding Box Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4b86a-33f3-47ef-95e1-61fd12820f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" SAVING BOUNDING BOX DETECTION MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create model name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"bbox_detector_{timestamp}\"\n",
    "model_path = os.path.join(MODEL_SAVE_DIR, model_name)\n",
    "\n",
    "print(f\" Saving model: {model_name}\")\n",
    "\n",
    "# Save in multiple formats\n",
    "try:\n",
    "    # 1. Save as .keras format (Keras 3 native format - recommended)\n",
    "    keras_model_path = f\"{model_path}.keras\"\n",
    "    model.save(keras_model_path)\n",
    "    print(f\"   ✅ Keras format: {keras_model_path}\")\n",
    "    \n",
    "    # 2. Export as SavedModel format (for TensorFlow Serving/TFLite)\n",
    "    savedmodel_path = f\"{model_path}_savedmodel\"\n",
    "    try:\n",
    "        model.export(savedmodel_path)\n",
    "        print(f\"   ✅ SavedModel format: {savedmodel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  SavedModel export failed: {e}\")\n",
    "        # Fallback: try using tf.saved_model.save\n",
    "        try:\n",
    "            tf.saved_model.save(model, savedmodel_path)\n",
    "            print(f\"   ✅ SavedModel format (fallback): {savedmodel_path}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ SavedModel save failed: {e2}\")\n",
    "\n",
    "    # 3. Save as H5 format (for backward compatibility)\n",
    "    h5_model_path = f\"{model_path}.h5\"\n",
    "    model.save(h5_model_path)\n",
    "    print(f\"   ✅ H5 format: {h5_model_path}\")\n",
    "\n",
    "    # 4. Save model weights only (Keras 3 compatible)\n",
    "    weights_path = f\"{model_path}.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"   ✅ Weights: {weights_path}\")\n",
    "    \n",
    "    # 4. Architecture JSON\n",
    "    architecture_path = f\"{model_path}_architecture.json\"\n",
    "    with open(architecture_path, 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "    print(f\"   ✅ Architecture: {architecture_path}\")\n",
    "    \n",
    "    # 5. Model metadata\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'tensorflow_version': tf.__version__,\n",
    "        'model_type': 'Bounding Box Regression CNN',\n",
    "        'task': 'License Plate Detection',\n",
    "        'input_shape': [IMG_SIZE, IMG_SIZE, 3],\n",
    "        'output_shape': [4],\n",
    "        'output_description': '[x_center, y_center, width, height] normalized',\n",
    "        'parameters': int(model.count_params()),\n",
    "        'training_results': training_results\n",
    "    }\n",
    "    \n",
    "    metadata_path = f\"{model_path}_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"   ✅ Metadata: {metadata_path}\")\n",
    "    zipf.writestr(f\"{model_name}_README.md\", readme_content)\n",
    "    \n",
    "    # Get file sizes\n",
    "    package_size = os.path.getsize(package_path) / (1024 * 1024)\n",
    "    keras_size = os.path.getsize(keras_model_path) / (1024 * 1024)\n",
    "    h5_size = os.path.getsize(h5_model_path) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\n📦 MODEL PACKAGE CREATED!\")\n",
    "    print(f\"   📁 Package: {model_name}\")\n",
    "    print(f\"   📊 Keras size: {keras_size:.2f} MB\")\n",
    "    print(f\"   📊 H5 size: {h5_size:.2f} MB\")\n",
    "    print(f\"   📦 Package size: {package_size:.2f} MB\")\n",
    "    print(f\"   🔢 Parameters: {model.count_params():,}\")\n",
    "    \n",
    "    print(f\"\\n🎯 DOWNLOAD READY!\")\n",
    "    print(f\"   📁 File: {package_path}\")\n",
    "    print(f\"   🚀 Load with: tf.keras.models.load_model('{model_name}.keras')\")\n",
    "    print(f\"   💡 Alternative: tf.keras.models.load_model('{model_name}.h5')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving model: {e}\")\n",
    "    print(\"   Model training completed but saving failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafb651-5741-42fc-9f12-2583e856efa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819ddb5-6575-443a-9465-cdcc1cacba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8155e6c-9cb3-4b1d-a8b1-a0b86cedf4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9827e01-808a-489f-b8ba-10b9f42f4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # 6. Create inference example\n",
    "    inference_code = f'''# License Plate Bounding Box Detection - Inference Example\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('{model_name}.h5')\n",
    "\n",
    "def predict_license_plate(image_path):\n",
    "    \"\"\"Predict license plate bounding box in an image\"\"\"\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    original_shape = image.shape[:2]\n",
    "    \n",
    "    # Resize to model input size\n",
    "    image_resized = cv2.resize(image, ({IMG_SIZE}, {IMG_SIZE}))\n",
    "    image_normalized = image_resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Add batch dimension and predict\n",
    "    image_batch = np.expand_dims(image_normalized, axis=0)\n",
    "    prediction = model.predict(image_batch)[0]\n",
    "    \n",
    "    # Convert normalized coordinates to original image coordinates\n",
    "    x_center, y_center, width, height = prediction\n",
    "    \n",
    "    # Convert to pixel coordinates\n",
    "    orig_h, orig_w = original_shape\n",
    "    x_center_px = x_center * orig_w\n",
    "    y_center_px = y_center * orig_h\n",
    "    width_px = width * orig_w\n",
    "    height_px = height * orig_h\n",
    "    \n",
    "    # Convert to corner coordinates\n",
    "    x1 = x_center_px - width_px / 2\n",
    "    y1 = y_center_px - height_px / 2\n",
    "    x2 = x_center_px + width_px / 2\n",
    "    y2 = y_center_px + height_px / 2\n",
    "    \n",
    "    return {{\n",
    "        'bbox_normalized': prediction,\n",
    "        'bbox_pixels': [x1, y1, x2, y2],\n",
    "        'center': [x_center_px, y_center_px],\n",
    "        'size': [width_px, height_px]\n",
    "    }}\n",
    "\n",
    "def visualize_prediction(image_path):\n",
    "    \"\"\"Visualize license plate detection result\"\"\"\n",
    "    \n",
    "    # Load original image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get prediction\n",
    "    result = predict_license_plate(image_path)\n",
    "    x1, y1, x2, y2 = result['bbox_pixels']\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2-x1, y2-y1,\n",
    "        linewidth=3, edgecolor='red', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title('License Plate Detection Result')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# result = predict_license_plate('your_image.jpg')\n",
    "# visualize_prediction('your_image.jpg')\n",
    "'''\n",
    "    \n",
    "    inference_path = f\"{model_path}_inference_example.py\"\n",
    "    with open(inference_path, 'w') as f:\n",
    "        f.write(inference_code)\n",
    "    print(f\"   ✅ Inference example: {inference_path}\")\n",
    "    \n",
    "    # 8. Create download package\n",
    "    package_path = f\"{model_path}_package.zip\"\n",
    "    with zipfile.ZipFile(package_path, 'w') as zipf:\n",
    "        zipf.write(keras_model_path, f\"{model_name}.keras\")\n",
    "        zipf.write(h5_model_path, f\"{model_name}.h5\")\n",
    "        zipf.write(weights_path, f\"{model_name}.weights.h5\")\n",
    "        zipf.write(architecture_path, f\"{model_name}_architecture.json\")\n",
    "        zipf.write(metadata_path, f\"{model_name}_metadata.json\")\n",
    "        zipf.write(inference_path, f\"{model_name}_inference_example.py\")\n",
    "        \n",
    "        # Add comprehensive README\n",
    "        readme_content = f'''# License Plate Bounding Box Detection Model\n",
    "\n",
    "## 🎯 Model Information\n",
    "- **Name**: {model_name}\n",
    "- **Type**: Bounding Box Regression CNN\n",
    "- **Task**: License Plate Detection (Coordinate Prediction)\n",
    "- **Created**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **TensorFlow Version**: {tf.__version__}\n",
    "\n",
    "## 📦 Files Included\n",
    "- `{model_name}.h5` - Complete model (recommended)\n",
    "- `{model_name}_weights.h5` - Model weights only\n",
    "- `{model_name}_architecture.json` - Model architecture\n",
    "- `{model_name}_metadata.json` - Training metadata\n",
    "- `{model_name}_inference_example.py` - Ready-to-use inference code\n",
    "\n",
    "## 🚀 Quick Start\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('{model_name}.h5')\n",
    "\n",
    "# Predict bounding box for an image\n",
    "image = preprocess_your_image()  # Shape: (1, {IMG_SIZE}, {IMG_SIZE}, 3)\n",
    "prediction = model.predict(image)  # Returns: [x_center, y_center, width, height]\n",
    "```\n",
    "\n",
    "## 📊 Model Performance\n",
    "{json.dumps(training_results, indent=2)}\n",
    "\n",
    "## 🔧 Model Specifications\n",
    "- **Input**: {IMG_SIZE}x{IMG_SIZE}x3 RGB images (normalized 0-1)\n",
    "- **Output**: 4 values [x_center, y_center, width, height] (normalized 0-1)\n",
    "- **Parameters**: {model.count_params():,}\n",
    "- **Loss Function**: Mean Squared Error (MSE)\n",
    "- **Metrics**: IoU (Intersection over Union), MAE\n",
    "\n",
    "## 📏 Coordinate Format\n",
    "- All coordinates are normalized to [0, 1] range\n",
    "- x_center, y_center: Center of bounding box\n",
    "- width, height: Size of bounding box\n",
    "- To convert to pixels: multiply by image dimensions\n",
    "\n",
    "## 🎯 Use Cases\n",
    "- License plate detection in traffic cameras\n",
    "- Automatic license plate recognition (ALPR) systems\n",
    "- Vehicle monitoring and tracking\n",
    "- Parking management systems\n",
    "\n",
    "## ⚡ Performance Tips\n",
    "- Input images should be well-lit and clear\n",
    "- Works best with frontal view license plates\n",
    "- Consider image preprocessing for better results\n",
    "- Use confidence thresholding based on your use case\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
