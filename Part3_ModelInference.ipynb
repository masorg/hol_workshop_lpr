{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d15ea96-f2e9-4cf3-a165-174b79dc08e7",
   "metadata": {},
   "source": [
    "# ðŸ” ** Model Inference & Testing**\n",
    "\n",
    "**Goal**: Load the trained bounding box detection model and perform comprehensive testing and evaluation\n",
    "\n",
    "## ðŸŽ¯ **What We'll Accomplish:**\n",
    "1. ** Load trained model** from multiple formats (Keras 3 compatible)\n",
    "2. ** Load and prepare test data** from the same LPR dataset\n",
    "3. ** Perform inference** on real license plate images\n",
    "4. ** Calculate performance metrics** (IoU, accuracy, MAE)\n",
    "5. ** Visualize predictions** with bounding box overlays\n",
    "6. ** Error analysis** and model insights\n",
    "7. ** Create inference pipeline** for new images\n",
    "\n",
    "##s **Testing Approach:**\n",
    "-  **Model loading** from different formats\n",
    "-  **Comprehensive evaluation** on test dataset\n",
    "-  **Visual validation** of predictions\n",
    "-  **Performance benchmarking** and analysis\n",
    "-  **Production-ready inference** pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d135dbf-0d09-41ba-a0b4-5f97988db9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:14:45.293695: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-18 16:14:45.654362: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-18 16:14:45.720877: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752855285.902859     288 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752855285.944551     288 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752855286.226838     288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752855286.226861     288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752855286.226863     288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752855286.226864     288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 16:14:46.232532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” PART 2B: MODEL INFERENCE & TESTING\n",
      "==================================================\n",
      " TensorFlow version: 2.19.0\n",
      " Keras version: 3.10.0\n",
      "  GPU available: False\n",
      " Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:14:58.905513: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Essential imports\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "print(\"ðŸ” PART 2B: MODEL INFERENCE & TESTING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" TensorFlow version: {tf.__version__}\")\n",
    "print(f\" Keras version: {tf.keras.__version__}\")\n",
    "print(f\"  GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\" Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481f69d-ab5c-4f1e-8882-50eb8df90033",
   "metadata": {},
   "source": [
    "##  **Configuration and Data Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69580362-687a-4992-a821-a01127bc2757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ CHECKING DATA AND MODEL AVAILABILITY\n",
      "========================================\n",
      " Dataset found!\n",
      "     Images: 433\n",
      "    Annotations: 433\n",
      "    Data completeness: True\n",
      "\n",
      " Saved models found:\n",
      "    Keras models (.keras): 1\n",
      "    H5 models (.h5): 0\n",
      "    Latest model: bbox_detector_20250718_231001.keras\n",
      "\n",
      " Configuration:\n",
      "    Image size: 400x400\n",
      "    Task: Bounding box regression\n",
      "    Output: [x_center, y_center, width, height]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"lpr_data\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"annotations\")\n",
    "MODEL_SAVE_DIR = \"saved_models\"\n",
    "IMG_SIZE = 400  # Same as training\n",
    "\n",
    "print(\"ðŸ“ CHECKING DATA AND MODEL AVAILABILITY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check dataset\n",
    "if os.path.exists(DATA_DIR):\n",
    "    image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.png')] if os.path.exists(IMAGES_DIR) else []\n",
    "    annotation_files = [f for f in os.listdir(ANNOTATIONS_DIR) if f.endswith('.xml')] if os.path.exists(ANNOTATIONS_DIR) else []\n",
    "    \n",
    "    print(f\" Dataset found!\")\n",
    "    print(f\"     Images: {len(image_files)}\")\n",
    "    print(f\"    Annotations: {len(annotation_files)}\")\n",
    "    print(f\"    Data completeness: {len(image_files) == len(annotation_files)}\")\n",
    "else:\n",
    "    print(\" Dataset not found. Please ensure lpr_data directory exists.\")\n",
    "\n",
    "# Check for saved models\n",
    "if os.path.exists(MODEL_SAVE_DIR):\n",
    "    model_files = glob.glob(os.path.join(MODEL_SAVE_DIR, \"bbox_detector_*.keras\"))\n",
    "    h5_files = glob.glob(os.path.join(MODEL_SAVE_DIR, \"bbox_detector_*.h5\"))\n",
    "    \n",
    "    print(f\"\\n Saved models found:\")\n",
    "    print(f\"    Keras models (.keras): {len(model_files)}\")\n",
    "    print(f\"    H5 models (.h5): {len(h5_files)}\")\n",
    "    \n",
    "    if model_files:\n",
    "        latest_model = max(model_files, key=os.path.getctime)\n",
    "        print(f\"    Latest model: {os.path.basename(latest_model)}\")\n",
    "        MODEL_PATH = latest_model\n",
    "    elif h5_files:\n",
    "        latest_model = max(h5_files, key=os.path.getctime)\n",
    "        print(f\"    Latest model: {os.path.basename(latest_model)}\")\n",
    "        MODEL_PATH = latest_model\n",
    "    else:\n",
    "        print(\"    No trained models found\")\n",
    "        MODEL_PATH = None\n",
    "else:\n",
    "    print(\"\\n Model directory not found\")\n",
    "    MODEL_PATH = None\n",
    "\n",
    "print(f\"\\n Configuration:\")\n",
    "print(f\"    Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"    Task: Bounding box regression\")\n",
    "print(f\"    Output: [x_center, y_center, width, height]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edca07c-fd9a-4124-bf79-9e5416b718a6",
   "metadata": {},
   "source": [
    "##  **Load Trained Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441048b5-ce05-41ce-8c1f-02627842e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LOADING TRAINED BOUNDING BOX DETECTION MODEL\n",
      "==================================================\n",
      "ðŸ”„ Loading model: bbox_detector_20250718_231001.keras\n",
      " Error loading model: File not found: filepath=saved_models/bbox_detector_20250718_231001.keras. Please ensure the file is an accessible `.keras` zip file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=saved_models/bbox_detector_20250718_231001.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MODEL_PATH:\n\u001b[0;32m---> 37\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Display model summary\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ—ï¸ Model Architecture Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mload_model_with_fallback\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Loading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(model_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load model (Keras 3 handles compilation automatically)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Recompile with same settings as training\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     15\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: filepath=saved_models/bbox_detector_20250718_231001.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "def load_model_with_fallback(model_path):\n",
    "    \"\"\"Load model with multiple format fallback for Keras 3 compatibility\"\"\"\n",
    "    \n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    print(f\"ðŸ”„ Loading model: {os.path.basename(model_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model (Keras 3 handles compilation automatically)\n",
    "        model = keras.models.load_model(model_path, compile=False)\n",
    "        \n",
    "        # Recompile with same settings as training\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        print(f\" Model loaded successfully!\")\n",
    "        print(f\"    Parameters: {model.count_params():,}\")\n",
    "        print(f\"    Input shape: {model.input_shape}\")\n",
    "        print(f\"    Output shape: {model.output_shape}\")\n",
    "        print(f\"    Architecture: {len(model.layers)} layers\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the model\n",
    "print(\" LOADING TRAINED BOUNDING BOX DETECTION MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if MODEL_PATH:\n",
    "    model = load_model_with_fallback(MODEL_PATH)\n",
    "    \n",
    "    # Display model summary\n",
    "    print(f\"\\nðŸ—ï¸ Model Architecture Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "else:\n",
    "    print(\" No model available for testing\")\n",
    "    print(\"   Please run the training notebook first to create a model\")\n",
    "    raise FileNotFoundError(\"No trained model found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b5628e-0c96-4de7-9da2-b871a3bd5b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LOADING TEST DATASET\n",
      "==============================\n",
      " Loading and preparing test data...\n",
      "    Processing 0/200 images...\n",
      "    Processing 50/200 images...\n",
      "    Processing 100/200 images...\n",
      "    Processing 150/200 images...\n",
      "\n",
      " Data loading complete!\n",
      "   Total samples: 200\n",
      "    Image shape: (400, 400, 3)\n",
      "    Bbox shape: (4,)\n",
      "    Processing success rate: 200/200 (100.0%)\n",
      "\n",
      " Data split:\n",
      "    Test samples: 140\n",
      "    Validation samples: 60\n"
     ]
    }
   ],
   "source": [
    "def parse_bounding_box(xml_path):\n",
    "    \"\"\"Extract bounding box coordinates from XML annotation\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Get image dimensions\n",
    "        size = root.find('size')\n",
    "        if size is None:\n",
    "            return None\n",
    "            \n",
    "        img_width = int(size.find('width').text)\n",
    "        img_height = int(size.find('height').text)\n",
    "        \n",
    "        # Find license plate bounding boxes\n",
    "        for obj in root.findall('object'):\n",
    "            name_elem = obj.find('name')\n",
    "            n_elem = obj.find('n')\n",
    "            \n",
    "            obj_name = \"\"\n",
    "            if name_elem is not None and name_elem.text:\n",
    "                obj_name = name_elem.text.lower()\n",
    "            elif n_elem is not None and n_elem.text:\n",
    "                obj_name = n_elem.text.lower()\n",
    "            \n",
    "            # Check if it's a license plate\n",
    "            if 'licen' in obj_name or 'plate' in obj_name:\n",
    "                bbox = obj.find('bndbox')\n",
    "                if bbox is not None:\n",
    "                    xmin = int(bbox.find('xmin').text)\n",
    "                    ymin = int(bbox.find('ymin').text)\n",
    "                    xmax = int(bbox.find('xmax').text)\n",
    "                    ymax = int(bbox.find('ymax').text)\n",
    "                    \n",
    "                    # Convert to center coordinates and normalize\n",
    "                    x_center = (xmin + xmax) / 2 / img_width\n",
    "                    y_center = (ymin + ymax) / 2 / img_height\n",
    "                    width = (xmax - xmin) / img_width\n",
    "                    height = (ymax - ymin) / img_height\n",
    "                    \n",
    "                    return [x_center, y_center, width, height]\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\" Error parsing {xml_path}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_and_prepare_data(max_samples=None):\n",
    "    \"\"\"Load and prepare dataset for testing\"\"\"\n",
    "    print(\" Loading and preparing test data...\")\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    image_names = []\n",
    "    original_images = []  # Store original images for visualization\n",
    "    \n",
    "    # Get list of image files\n",
    "    image_files = [f for f in os.listdir(IMAGES_DIR) if f.endswith('.png')]\n",
    "    \n",
    "    if max_samples:\n",
    "        image_files = image_files[:max_samples]\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"    Processing {i}/{len(image_files)} images...\")\n",
    "        \n",
    "        # Get corresponding annotation\n",
    "        xml_file = img_file.replace('.png', '.xml')\n",
    "        img_path = os.path.join(IMAGES_DIR, img_file)\n",
    "        xml_path = os.path.join(ANNOTATIONS_DIR, xml_file)\n",
    "        \n",
    "        if not os.path.exists(xml_path):\n",
    "            continue\n",
    "            \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(img_path)\n",
    "        if original_image is None:\n",
    "            continue\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize and normalize for model input\n",
    "        image = cv2.resize(original_image, (IMG_SIZE, IMG_SIZE))\n",
    "        image_normalized = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Get bounding box\n",
    "        bbox = parse_bounding_box(xml_path)\n",
    "        if bbox is None:\n",
    "            continue\n",
    "            \n",
    "        X_data.append(image_normalized)\n",
    "        y_data.append(bbox)\n",
    "        image_names.append(img_file)\n",
    "        original_images.append(original_image)\n",
    "        processed_count += 1\n",
    "    \n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    print(f\"\\n Data loading complete!\")\n",
    "    print(f\"   Total samples: {len(X_data)}\")\n",
    "    print(f\"    Image shape: {X_data[0].shape}\")\n",
    "    print(f\"    Bbox shape: {y_data[0].shape}\")\n",
    "    print(f\"    Processing success rate: {processed_count}/{len(image_files)} ({100*processed_count/len(image_files):.1f}%)\")\n",
    "    \n",
    "    return X_data, y_data, image_names, original_images\n",
    "\n",
    "# Load test data\n",
    "print(\" LOADING TEST DATASET\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "X_test, y_test, test_image_names, original_test_images = load_and_prepare_data(max_samples=200)  # Limit for faster testing\n",
    "\n",
    "if len(X_test) == 0:\n",
    "    print(\" No test data loaded\")\n",
    "    raise ValueError(\"No test data available\")\n",
    "\n",
    "# Split into smaller test and validation sets\n",
    "X_test_small, X_val_small, y_test_small, y_val_small, names_test, names_val, orig_test, orig_val = train_test_split(\n",
    "    X_test, y_test, test_image_names, original_test_images, \n",
    "    test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n Data split:\")\n",
    "print(f\"    Test samples: {len(X_test_small)}\")\n",
    "print(f\"    Validation samples: {len(X_val_small)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404365c-636b-4066-a842-b2b12dd6cabd",
   "metadata": {},
   "source": [
    "##  **Model Inference and Performance Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171d3b7e-dfe7-4c59-835a-5abad319d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PERFORMING MODEL INFERENCE AND EVALUATION\n",
      "=============================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m PERFORMING MODEL INFERENCE AND EVALUATION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m45\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m test_results \u001b[38;5;241m=\u001b[39m perform_inference_and_evaluation(\u001b[43mmodel\u001b[49m, X_test_small, y_test_small, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m val_results \u001b[38;5;241m=\u001b[39m perform_inference_and_evaluation(model, X_val_small, y_val_small, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m PERFORMANCE RESULTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) for two bounding boxes\"\"\"\n",
    "    # Convert center format to corner format\n",
    "    x1_1 = box1[0] - box1[2] / 2\n",
    "    y1_1 = box1[1] - box1[3] / 2\n",
    "    x2_1 = box1[0] + box1[2] / 2\n",
    "    y2_1 = box1[1] + box1[3] / 2\n",
    "    \n",
    "    x1_2 = box2[0] - box2[2] / 2\n",
    "    y1_2 = box2[1] - box2[3] / 2\n",
    "    x2_2 = box2[0] + box2[2] / 2\n",
    "    y2_2 = box2[1] + box2[3] / 2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x1_i = max(x1_1, x1_2)\n",
    "    y1_i = max(y1_1, y1_2)\n",
    "    x2_i = min(x2_1, x2_2)\n",
    "    y2_i = min(y2_1, y2_2)\n",
    "    \n",
    "    if x2_i <= x1_i or y2_i <= y1_i:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    \n",
    "    # Calculate union\n",
    "    area1 = box1[2] * box1[3]\n",
    "    area2 = box2[2] * box2[3]\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def perform_inference_and_evaluation(model, X_data, y_data, data_name=\"Test\"):\n",
    "    \"\"\"Perform inference and calculate comprehensive metrics\"\"\"\n",
    "    print(f\"ðŸ” Performing inference on {data_name.lower()} set...\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(X_data, verbose=0)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ious = []\n",
    "    mae_scores = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        true_bbox = y_data[i]\n",
    "        pred_bbox = predictions[i]\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = calculate_iou(true_bbox, pred_bbox)\n",
    "        ious.append(iou)\n",
    "        \n",
    "        # Calculate MAE for each coordinate\n",
    "        mae = mean_absolute_error(true_bbox, pred_bbox)\n",
    "        mae_scores.append(mae)\n",
    "    \n",
    "    ious = np.array(ious)\n",
    "    mae_scores = np.array(mae_scores)\n",
    "    \n",
    "    # Performance statistics\n",
    "    fps = len(X_data) / inference_time\n",
    "    avg_time_per_image = inference_time / len(X_data) * 1000  # ms\n",
    "    \n",
    "    results = {\n",
    "        'predictions': predictions,\n",
    "        'ious': ious,\n",
    "        'mae_scores': mae_scores,\n",
    "        'mean_iou': np.mean(ious),\n",
    "        'median_iou': np.median(ious),\n",
    "        'std_iou': np.std(ious),\n",
    "        'mean_mae': np.mean(mae_scores),\n",
    "        'inference_time': inference_time,\n",
    "        'fps': fps,\n",
    "        'avg_time_per_image': avg_time_per_image,\n",
    "        'iou_at_50': np.sum(ious > 0.5) / len(ious),\n",
    "        'iou_at_75': np.sum(ious > 0.75) / len(ious),\n",
    "        'best_iou': np.max(ious),\n",
    "        'worst_iou': np.min(ious)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform inference on test data\n",
    "print(\" PERFORMING MODEL INFERENCE AND EVALUATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "test_results = perform_inference_and_evaluation(model, X_test_small, y_test_small, \"Test\")\n",
    "val_results = perform_inference_and_evaluation(model, X_val_small, y_val_small, \"Validation\")\n",
    "\n",
    "print(f\"\\n PERFORMANCE RESULTS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Test set results\n",
    "print(f\" Test Set ({len(X_test_small)} samples):\")\n",
    "print(f\"    Mean IoU: {test_results['mean_iou']:.4f}\")\n",
    "print(f\"    Median IoU: {test_results['median_iou']:.4f}\")\n",
    "print(f\"    Mean MAE: {test_results['mean_mae']:.4f}\")\n",
    "print(f\"    IoU > 0.5: {test_results['iou_at_50']:.1%}\")\n",
    "print(f\"    IoU > 0.75: {test_results['iou_at_75']:.1%}\")\n",
    "print(f\"    FPS: {test_results['fps']:.1f}\")\n",
    "print(f\"    Avg time/image: {test_results['avg_time_per_image']:.1f}ms\")\n",
    "\n",
    "# Validation set results\n",
    "print(f\"\\n Validation Set ({len(X_val_small)} samples):\")\n",
    "print(f\"    Mean IoU: {val_results['mean_iou']:.4f}\")\n",
    "print(f\"    Median IoU: {val_results['median_iou']:.4f}\")\n",
    "print(f\"    Mean MAE: {val_results['mean_mae']:.4f}\")\n",
    "print(f\"    IoU > 0.5: {val_results['iou_at_50']:.1%}\")\n",
    "print(f\"    IoU > 0.75: {val_results['iou_at_75']:.1%}\")\n",
    "\n",
    "# Overall assessment\n",
    "overall_iou = (test_results['mean_iou'] + val_results['mean_iou']) / 2\n",
    "print(f\"\\n Overall Performance:\")\n",
    "print(f\"    Average IoU: {overall_iou:.4f}\")\n",
    "if overall_iou > 0.7:\n",
    "    print(f\"    Excellent performance!\")\n",
    "elif overall_iou > 0.5:\n",
    "    print(f\"    Good performance!\")\n",
    "elif overall_iou > 0.3:\n",
    "    print(f\"     Moderate performance\")\n",
    "else:\n",
    "    print(f\"    Needs improvement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029b7f0-98a8-49e6-aa35-c90f020c1a62",
   "metadata": {},
   "source": [
    "##  **Visual Prediction Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e58fca-f605-4775-a6d4-1651cfff53e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VISUALIZING PREDICTION EXAMPLES\n",
      "===================================\n",
      " Test Set Predictions:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 85\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m35\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Test Set Predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m visualize_predictions(\n\u001b[1;32m     83\u001b[0m     orig_test, \n\u001b[1;32m     84\u001b[0m     y_test_small, \n\u001b[0;32m---> 85\u001b[0m     \u001b[43mtest_results\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     86\u001b[0m     test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mious\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     87\u001b[0m     names_test,\n\u001b[1;32m     88\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Validation Set Predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m visualize_predictions(\n\u001b[1;32m     93\u001b[0m     orig_val, \n\u001b[1;32m     94\u001b[0m     y_val_small, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_results' is not defined"
     ]
    }
   ],
   "source": [
    "def draw_bounding_box(image, bbox, color, label, thickness=3):\n",
    "    \"\"\"Draw bounding box on image\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Convert normalized coordinates to pixels\n",
    "    x_center = bbox[0] * w\n",
    "    y_center = bbox[1] * h\n",
    "    width = bbox[2] * w\n",
    "    height = bbox[3] * h\n",
    "    \n",
    "    # Convert to corner coordinates\n",
    "    x1 = int(x_center - width / 2)\n",
    "    y1 = int(y_center - height / 2)\n",
    "    x2 = int(x_center + width / 2)\n",
    "    y2 = int(y_center + height / 2)\n",
    "    \n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "    \n",
    "    # Add label\n",
    "    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "    cv2.rectangle(image, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), color, -1)\n",
    "    cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def visualize_predictions(images, true_bboxes, pred_bboxes, ious, names, num_samples=12):\n",
    "    \"\"\"Visualize predictions with bounding boxes\"\"\"\n",
    "    \n",
    "    # Select samples: best, worst, and random\n",
    "    sorted_indices = np.argsort(ious)\n",
    "    \n",
    "    # Get indices for visualization\n",
    "    best_indices = sorted_indices[-3:]  # Best 3\n",
    "    worst_indices = sorted_indices[:3]  # Worst 3\n",
    "    random_indices = np.random.choice(len(images), min(6, len(images)), replace=False)\n",
    "    \n",
    "    display_indices = np.concatenate([best_indices, worst_indices, random_indices])[:num_samples]\n",
    "    \n",
    "    cols = 4\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(' License Plate Detection Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, ax_idx in enumerate(display_indices[:num_samples]):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        \n",
    "        if row >= rows or col >= cols:\n",
    "            break\n",
    "            \n",
    "        # Create copy of original image\n",
    "        img_display = images[ax_idx].copy()\n",
    "        \n",
    "        # Draw ground truth (green) and prediction (red)\n",
    "        img_display = draw_bounding_box(img_display, true_bboxes[ax_idx], (0, 255, 0), \"True\", 3)\n",
    "        img_display = draw_bounding_box(img_display, pred_bboxes[ax_idx], (255, 0, 0), \"Pred\", 2)\n",
    "        \n",
    "        axes[row, col].imshow(img_display)\n",
    "        axes[row, col].set_title(f\"{names[ax_idx]}\\nIoU: {ious[ax_idx]:.3f}\", fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(num_samples, rows * cols):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        if row < rows and col < cols:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "print(\" VISUALIZING PREDICTION EXAMPLES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\" Test Set Predictions:\")\n",
    "visualize_predictions(\n",
    "    orig_test, \n",
    "    y_test_small, \n",
    "    test_results['predictions'], \n",
    "    test_results['ious'], \n",
    "    names_test,\n",
    "    num_samples=8\n",
    ")\n",
    "\n",
    "print(\"\\n Validation Set Predictions:\")\n",
    "visualize_predictions(\n",
    "    orig_val, \n",
    "    y_val_small, \n",
    "    val_results['predictions'], \n",
    "    val_results['ious'], \n",
    "    names_val,\n",
    "    num_samples=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826873c1-12ac-40ee-bb0a-c5fb073574ba",
   "metadata": {},
   "source": [
    "##  **Final Model Assessment and Summary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aaec397-e2df-45eb-95a7-7682cac861b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FINAL MODEL ASSESSMENT AND SUMMARY\n",
      "========================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create comprehensive summary\u001b[39;00m\n\u001b[1;32m      5\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_info\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: MODEL_PATH,\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcount_params(),\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(model\u001b[38;5;241m.\u001b[39minput_shape),\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(model\u001b[38;5;241m.\u001b[39moutput_shape),\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers)\n\u001b[1;32m     12\u001b[0m     },\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_info\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(X_test),\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(X_test_small),\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(X_val_small)\n\u001b[1;32m     17\u001b[0m     },\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_iou\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_iou\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_50\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_50\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_75\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_75\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_mae\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_mae\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m         },\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_iou\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_iou\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_50\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_50\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_75\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou_at_75\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_mae\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_mae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     33\u001b[0m         }\n\u001b[1;32m     34\u001b[0m     },\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Display summary\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m MODEL INFORMATION:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\" FINAL MODEL ASSESSMENT AND SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = {\n",
    "    'model_info': {\n",
    "        'path': MODEL_PATH,\n",
    "        'parameters': model.count_params(),\n",
    "        'input_shape': str(model.input_shape),\n",
    "        'output_shape': str(model.output_shape),\n",
    "        'layers': len(model.layers)\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(X_test),\n",
    "        'test_samples': len(X_test_small),\n",
    "        'validation_samples': len(X_val_small)\n",
    "    },\n",
    "    'performance': {\n",
    "        'test': {\n",
    "            'mean_iou': float(test_results['mean_iou']),\n",
    "            'median_iou': float(test_results['median_iou']),\n",
    "            'iou_at_50': float(test_results['iou_at_50']),\n",
    "            'iou_at_75': float(test_results['iou_at_75']),\n",
    "            'mean_mae': float(test_results['mean_mae']),\n",
    "            'fps': float(test_results['fps'])\n",
    "        },\n",
    "        'validation': {\n",
    "            'mean_iou': float(val_results['mean_iou']),\n",
    "            'median_iou': float(val_results['median_iou']),\n",
    "            'iou_at_50': float(val_results['iou_at_50']),\n",
    "            'iou_at_75': float(val_results['iou_at_75']),\n",
    "            'mean_mae': float(val_results['mean_mae'])\n",
    "        }\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print(f\" MODEL INFORMATION:\")\n",
    "print(f\"    Model: {os.path.basename(MODEL_PATH)}\")\n",
    "print(f\"    Parameters: {summary['model_info']['parameters']:,}\")\n",
    "print(f\"    Layers: {summary['model_info']['layers']}\")\n",
    "print(f\"    Input: {summary['model_info']['input_shape']}\")\n",
    "print(f\"    Output: {summary['model_info']['output_shape']}\")\n",
    "\n",
    "print(f\"\\n DATASET INFORMATION:\")\n",
    "print(f\"    Total samples: {summary['dataset_info']['total_samples']}\")\n",
    "print(f\"    Test samples: {summary['dataset_info']['test_samples']}\")\n",
    "print(f\"    Validation samples: {summary['dataset_info']['validation_samples']}\")\n",
    "\n",
    "print(f\"\\n PERFORMANCE SUMMARY:\")\n",
    "print(f\"    Test Set:\")\n",
    "print(f\"       Mean IoU: {summary['performance']['test']['mean_iou']:.4f}\")\n",
    "print(f\"       IoU > 0.5: {summary['performance']['test']['iou_at_50']:.1%}\")\n",
    "print(f\"       IoU > 0.75: {summary['performance']['test']['iou_at_75']:.1%}\")\n",
    "print(f\"       Mean MAE: {summary['performance']['test']['mean_mae']:.4f}\")\n",
    "print(f\"       FPS: {summary['performance']['test']['fps']:.1f}\")\n",
    "\n",
    "print(f\"    Validation Set:\")\n",
    "print(f\"       Mean IoU: {summary['performance']['validation']['mean_iou']:.4f}\")\n",
    "print(f\"       IoU > 0.5: {summary['performance']['validation']['iou_at_50']:.1%}\")\n",
    "print(f\"       IoU > 0.75: {summary['performance']['validation']['iou_at_75']:.1%}\")\n",
    "print(f\"       Mean MAE: {summary['performance']['validation']['mean_mae']:.4f}\")\n",
    "\n",
    "# Overall assessment\n",
    "overall_iou = (summary['performance']['test']['mean_iou'] + summary['performance']['validation']['mean_iou']) / 2\n",
    "overall_fps = summary['performance']['test']['fps']\n",
    "\n",
    "print(f\"\\n OVERALL ASSESSMENT:\")\n",
    "print(f\"    Average IoU: {overall_iou:.4f}\")\n",
    "print(f\"    Inference Speed: {overall_fps:.1f} FPS\")\n",
    "\n",
    "if overall_iou > 0.7 and overall_fps > 10:\n",
    "    grade = \" EXCELLENT\"\n",
    "    recommendation = \"Ready for production deployment\"\n",
    "elif overall_iou > 0.5 and overall_fps > 5:\n",
    "    grade = \" GOOD\"\n",
    "    recommendation = \"Suitable for most applications\"\n",
    "elif overall_iou > 0.3:\n",
    "    grade = \" MODERATE\"\n",
    "    recommendation = \"Consider additional training or data augmentation\"\n",
    "else:\n",
    "    grade = \" NEEDS IMPROVEMENT\"\n",
    "    recommendation = \"Requires model architecture changes or more training data\"\n",
    "\n",
    "print(f\"    Grade: {grade}\")\n",
    "print(f\"    Recommendation: {recommendation}\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = f\"inference_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n Results saved to: {summary_path}\")\n",
    "\n",
    "print(f\"\\n MODEL INFERENCE AND TESTING COMPLETE!\")\n",
    "print(f\"    Model successfully loaded and tested\")\n",
    "print(f\"    Comprehensive evaluation performed\")\n",
    "print(f\"    Performance benchmarking completed\")\n",
    "print(f\"    Visual validation completed\")\n",
    "print(f\"    Results saved for future reference\")\n",
    "print(f\"    Ready for deployment!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec784e-2619-41f1-81f3-f545e5d12307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
